# model:
name: "finetuning_transformer"
d_model: 256
d_ff: 1024
num_heads: 8
num_layers: 8
dropout: 0.2
pos_encoding: fixed
activation: relu
normalization_layer: BatchNorm
freeze: False
l2_reg: 0
global_reg: False
# training:
epochs: 500
batch_size: 8
num_workers: 4
lr: 0.00001
patience: 10
# evaluation:
beta: 2
weights_file: "/usr/stud/roschman/ECGAnalysis/physionet_evaluation/weights.csv"
output_dir: "/usr/stud/roschman/ECGAnalysis/output"
task: "classification"